{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For interacting with the operating system (e.g., directory and file operations)\n",
    "import shutil  # For file and directory manipulation (e.g., copying or moving files)\n",
    "import xml.etree.ElementTree as ET  # For parsing and working with XML files\n",
    "from sklearn.model_selection import train_test_split  # For splitting datasets into train, test, and validation sets\n",
    "import yaml  # For reading and writing YAML configuration files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_structure(base_path):\n",
    "    # Creates the folder structure for storing training, validation, and test images and labels\n",
    "    folders = [\n",
    "        'data/train/images', 'data/train/labels',\n",
    "        'data/val/images', 'data/val/labels',\n",
    "        'data/test/images', 'data/test/labels'\n",
    "    ]\n",
    "    for folder in folders:\n",
    "        # Create each folder if it doesn't already exist\n",
    "        os.makedirs(os.path.join(base_path, folder), exist_ok=True)\n",
    "\n",
    "def convert_bbox(size, box):\n",
    "    # Converts bounding box coordinates from absolute to YOLO format (normalized)\n",
    "    dw = 1. / size[0]  # Width scaling factor\n",
    "    dh = 1. / size[1]  # Height scaling factor\n",
    "    x = (box[0] + box[2]) / 2.0  # Calculate the center x-coordinate\n",
    "    y = (box[1] + box[3]) / 2.0  # Calculate the center y-coordinate\n",
    "    w = box[2] - box[0]  # Calculate the width of the bounding box\n",
    "    h = box[3] - box[1]  # Calculate the height of the bounding box\n",
    "    return (x * dw, y * dh, w * dw, h * dh)  # Return normalized coordinates\n",
    "\n",
    "def convert_annotation(xml_path, output_path, classes):\n",
    "    # Converts annotation data from XML format to YOLO format\n",
    "    tree = ET.parse(xml_path)  # Parse the XML file\n",
    "    root = tree.getroot()  # Get the root of the XML tree\n",
    "    size = root.find('size')  # Find the image size element\n",
    "    w = int(size.find('width').text)  # Extract image width\n",
    "    h = int(size.find('height').text)  # Extract image height\n",
    "\n",
    "    # Open the output text file for writing annotations\n",
    "    with open(output_path, 'w') as out_file:\n",
    "        for obj in root.iter('object'):  # Iterate over each object in the XML file\n",
    "            difficult = obj.find('difficult').text  # Check if the object is marked as difficult\n",
    "            cls = obj.find('name').text  # Get the class name of the object\n",
    "            if cls not in classes or int(difficult) == 1:\n",
    "                # Skip the object if its class is not in the list or if it is marked difficult\n",
    "                continue\n",
    "            cls_id = classes.index(cls)  # Get the index of the class in the classes list\n",
    "            xmlbox = obj.find('bndbox')  # Find the bounding box coordinates\n",
    "            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text),\n",
    "                 float(xmlbox.find('xmax').text), float(xmlbox.find('ymax').text))  # Extract bounding box values\n",
    "            bb = convert_bbox((w, h), b)  # Convert bounding box to YOLO format\n",
    "            # Write the class ID and normalized bounding box to the output file\n",
    "            out_file.write(f\"{cls_id} {' '.join(map(str, bb))}\\n\")\n",
    "\n",
    "def process_images(image_files, image_folder, annotation_folder, output_path, split, classes):\n",
    "    # Processes images and corresponding annotations for a given dataset split\n",
    "    for file in image_files:\n",
    "        # Handle image processing\n",
    "        src_img = os.path.join(image_folder, file)  # Path to the source image\n",
    "        dst_img = os.path.join(output_path, f'data/{split}/images', file)  # Destination for the image\n",
    "        shutil.copy(src_img, dst_img)  # Copy the image to the destination folder\n",
    "\n",
    "        # Handle annotation processing\n",
    "        xml_file = os.path.splitext(file)[0] + '.xml'  # Replace file extension to match XML annotation\n",
    "        src_xml = os.path.join(annotation_folder, xml_file)  # Path to the source annotation file\n",
    "        dst_txt = os.path.join(output_path, f'data/{split}/labels', os.path.splitext(file)[0] + '.txt')  # Destination for the annotation\n",
    "        convert_annotation(src_xml, dst_txt, classes)  # Convert and save the annotation in YOLO format\n",
    "\n",
    "def split_and_process_dataset(image_files, image_folder, annotation_folder, output_path, classes):\n",
    "    # Splits the dataset into training, validation, and testing sets, and processes each split\n",
    "    train_val, test = train_test_split(image_files, test_size=0.2, random_state=42)  # Split into train+val and test\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=42)  # Further split train+val into train and val\n",
    "\n",
    "    # Process each dataset split\n",
    "    for split, files in [('train', train), ('val', val), ('test', test)]:\n",
    "        process_images(files, image_folder, annotation_folder, output_path, split, classes)\n",
    "\n",
    "def create_yaml(output_path, classes):\n",
    "    # Creates a YAML configuration file for the dataset\n",
    "    yaml_content = {\n",
    "        'train': f'{HOME}/data/train/images',  # Path to training images\n",
    "        'val': f'{HOME}/data/val/images',  # Path to validation images\n",
    "        'test': f'{HOME}/data/test/images',  # Path to testing images\n",
    "        'nc': len(classes),  # Number of classes in the dataset\n",
    "        'names': classes  # List of class names\n",
    "    }\n",
    "\n",
    "    # Write the YAML content to a file\n",
    "    with open(os.path.join(output_path, 'data', 'data.yaml'), 'w') as yaml_file:\n",
    "        yaml.dump(yaml_content, yaml_file, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/FaceMaskData"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set the HOME variable to the current working directory\n",
    "    HOME = os.getcwd()\n",
    "    print(HOME)  # Print the HOME path for verification\n",
    "\n",
    "    # Create the folder structure for storing processed dataset\n",
    "    create_folder_structure(HOME)\n",
    "\n",
    "    # Define the path to the dataset and the output directory\n",
    "    dataset_path = 'Data/FaceMaskData'  # Path to the dataset containing images and annotations\n",
    "    output_path = f'{HOME}/'  # Output directory for processed data\n",
    "\n",
    "    # Define the classes present in the dataset\n",
    "    classes = ['with_mask', 'without_mask', 'mask_weared_incorrect']\n",
    "\n",
    "    # Define the paths to the image and annotation folders\n",
    "    image_folder = os.path.join(dataset_path, 'images')  # Path to the folder containing images\n",
    "    annotation_folder = os.path.join(dataset_path, 'annotations')  # Path to the folder containing annotations\n",
    "\n",
    "    # List all image files in the image folder that have a .png extension\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets, and process them\n",
    "    split_and_process_dataset(image_files, image_folder, annotation_folder, output_path, classes)\n",
    "\n",
    "    # Create the YAML file for dataset configuration\n",
    "    create_yaml(output_path, classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (8.3.15)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (1.14.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Use the pip package manager to install or update the Ultralytics library to the latest version.\n",
    "# The Ultralytics library provides tools for YOLO-based object detection and training models.\n",
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.15 🚀 Python-3.11.0 torch-2.4.1 CPU (Apple M1)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/Users/tanishapriya/Desktop/Computer_Vision/data/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "Model summary: 249 layers, 9,840,121 parameters, 9,840,105 gradients, 23.6 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/tanishapriya/Desktop/Computer_Vision/data/train/labels.cache... 545 images, 242 backgrounds, 0 corrupt: 100%|██████████| 787/787 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/tanishapriya/Desktop/Computer_Vision/data/val/labels.cache... 137 images, 65 backgrounds, 0 corrupt: 100%|██████████| 202/202 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.919      4.048      1.568         97        640:  80%|████████  | 20/25 [32:13<04:18, 51.78s/it]   libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       1/10         0G      1.812      3.697      1.486         57        640: 100%|██████████| 25/25 [35:11<00:00, 84.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:06<00:00, 16.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579    0.00262      0.263     0.0629     0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.486      2.459      1.161         75        640:  24%|██▍       | 6/25 [04:19<14:12, 44.87s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       2/10         0G      1.342      2.196      1.124         46        640: 100%|██████████| 25/25 [22:46<00:00, 54.65s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:06<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579    0.00194      0.205    0.00731    0.00302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.293      1.987      1.105        101        640:  24%|██▍       | 6/25 [09:58<19:07, 60.42s/it]   libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       3/10         0G      1.292      1.954      1.097         73        640: 100%|██████████| 25/25 [22:57<00:00, 55.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:10<00:00, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579        0.9     0.0199      0.073     0.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.245      1.709      1.066        108        640:  84%|████████▍ | 21/25 [22:22<07:58, 119.74s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       4/10         0G      1.234      1.719      1.063         57        640: 100%|██████████| 25/25 [24:39<00:00, 59.17s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:02<00:00, 15.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.272     0.0806      0.199      0.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.235      1.613      1.037        169        640:  48%|████▊     | 12/25 [07:48<08:30, 39.23s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       5/10         0G      1.237      1.608      1.051         39        640: 100%|██████████| 25/25 [16:08<00:00, 38.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:01<00:00, 15.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579       0.31      0.183      0.233      0.148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.171      1.435       1.05         92        640:   4%|▍         | 1/25 [00:38<15:24, 38.54s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       6/10         0G      1.233      1.491      1.059         33        640: 100%|██████████| 25/25 [17:42<00:00, 42.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:06<00:00, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579       0.32      0.364      0.309      0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.184      1.348      1.022         84        640:  92%|█████████▏| 23/25 [16:47<01:30, 45.39s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       7/10         0G      1.175      1.345      1.018         56        640: 100%|██████████| 25/25 [18:01<00:00, 43.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:12<00:00, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.689      0.482      0.344       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.113      1.291      1.001        104        640:  64%|██████▍   | 16/25 [12:55<06:24, 42.77s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       8/10         0G       1.12      1.248      1.003         95        640: 100%|██████████| 25/25 [18:51<00:00, 45.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:07<00:00, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.357      0.504      0.352      0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.102      1.177     0.9911        105        640:  68%|██████▊   | 17/25 [12:00<05:48, 43.54s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "       9/10         0G      1.105      1.165     0.9858        204        640: 100%|██████████| 25/25 [18:47<00:00, 45.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:24<00:00, 21.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.705      0.497      0.357      0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.074      1.143     0.9756         91        640:  84%|████████▍ | 21/25 [17:58<03:16, 49.21s/it]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "      10/10         0G      1.065      1.138     0.9686         44        640: 100%|██████████| 25/25 [21:00<00:00, 50.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:14<00:00, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.709      0.525      0.383      0.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 3.799 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 19.9MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 19.9MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics 8.3.15 🚀 Python-3.11.0 torch-2.4.1 CPU (Apple M1)\n",
      "Model summary (fused): 186 layers, 9,828,825 parameters, 0 gradients, 23.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [01:06<00:00, 16.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579       0.71      0.527      0.383      0.255\n",
      "             with_mask        122        419      0.586      0.901      0.577      0.412\n",
      "          without_mask         47        145      0.546      0.679      0.493      0.303\n",
      " mask_weared_incorrect         14         15          1          0     0.0804     0.0485\n",
      "Speed: 0.8ms preprocess, 315.9ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO  # Import the YOLO class from the Ultralytics library\n",
    "\n",
    "# Load the pre-trained YOLOv8 small model (yolov8s.pt)\n",
    "model = YOLO('yolov8s.pt')  \n",
    "\n",
    "# Train the YOLO model using the specified configuration\n",
    "res = model.train(\n",
    "    data='Data/data.yaml',  # Path to the dataset configuration file (YAML format)\n",
    "    epochs=10,              # Number of training epochs (complete passes through the dataset)\n",
    "    batch=32,               # Batch size (number of samples processed at a time during training)\n",
    "    plots=True,             # Enable plots to visualize training metrics and progress\n",
    "    imgsz=640,              # Image size (resize input images to 640x640 pixels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting supervision\n",
      "  Downloading supervision-0.24.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: ultralytics in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (8.3.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from supervision)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from supervision) (4.10.0.84)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from supervision) (6.0.2)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from supervision) (1.14.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from ultralytics) (2.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
      "Requirement already satisfied: filelock in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tanishapriya/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading supervision-0.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: opencv-python-headless, defusedxml, supervision\n",
      "Successfully installed defusedxml-0.7.1 opencv-python-headless-4.10.0.84 supervision-0.24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the required Python libraries using pip.\n",
    "# These libraries are necessary for tasks like image processing, visualization, numerical computations, and YOLO-based object detection.\n",
    "\n",
    "pip install opencv-python-headless matplotlib numpy supervision ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.15 🚀 Python-3.11.0 torch-2.4.1 CPU (Apple M1)\n",
      "Model summary (fused): 186 layers, 9,828,825 parameters, 0 gradients, 23.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/tanishapriya/Desktop/Computer_Vision/data/val/labels.cache... 137 images, 65 backgrounds, 0 corrupt: 100%|██████████| 202/202 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:57<00:00,  8.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202        579      0.709      0.528      0.382      0.253\n",
      "             with_mask        122        419      0.584      0.902      0.578      0.412\n",
      "          without_mask         47        145      0.542      0.683      0.492      0.301\n",
      " mask_weared_incorrect         14         15          1          0     0.0747     0.0458\n",
      "Speed: 1.3ms preprocess, 269.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train42\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained YOLO model on the validation/test dataset\n",
    "results = model.val(\n",
    "    data='Data/data.yaml',  # Path to the dataset configuration file (YAML format)\n",
    "    batch=32,               # Batch size (number of samples processed at a time during evaluation)\n",
    "    imgsz=640,              # Image size (resize input images to 640x640 pixels for evaluation)\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
