{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\Coding\\Python\\AI\\PreProd\\Week4DIY\\DIY_TransferLearning\\Data\\TwitterData/twitter_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75682 entries, 0 to 75681\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweetID        75682 non-null  int64 \n",
      " 1   entity         75682 non-null  object\n",
      " 2   sentiment      75682 non-null  object\n",
      " 3   tweet_content  74996 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       entity sentiment  \\\n",
       "0     2401  Borderlands  Positive   \n",
       "1     2401  Borderlands  Positive   \n",
       "2     2401  Borderlands  Positive   \n",
       "3     2401  Borderlands  Positive   \n",
       "4     2401  Borderlands  Positive   \n",
       "\n",
       "                                       tweet_content  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75682, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed Text'] = data['tweet_content'].apply(preprocess) \n",
    "    # Encode categorical columns\n",
    "le = LabelEncoder() # Initialize the LabelEncoder\n",
    "data['sentiment'] = le.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TFBertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed Text'] = data['Preprocessed Text'].astype(str)  # Convert all to string\n",
    "data = data[data['Preprocessed Text'].str.strip() != '']  # Drop empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prach\\AppData\\Local\\Temp\\ipykernel_15732\\2789582200.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=5000/75682, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "data = data.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=5000/75682, random_state=42))\n",
    "X = data[\"Preprocessed Text\"]\n",
    "y = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation (50%) and test (50%) to get 10% each\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 3724, Length of y_train: 3724\n",
      "Length of X_val: 466, Length of y_val: 466\n",
      "Length of X_test: 466, Length of y_test: 466\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of X_train: {len(X_train)}, Length of y_train: {len(y_train)}\")\n",
    "print(f\"Length of X_val: {len(X_val)}, Length of y_val: {len(y_val)}\")\n",
    "print(f\"Length of X_test: {len(X_test)}, Length of y_test: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and encode the data using the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 128\n",
    "# Tokenize and encode the sentences\n",
    "X_train_encoded = tokenizer.batch_encode_plus(X.tolist(),\n",
    "\t\t\t\t\t\t\t\t\t\t\tpadding=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmax_length = max_len,\n",
    "\t\t\t\t\t\t\t\t\t\t\treturn_tensors='tf')\n",
    "\n",
    "X_val_encoded = tokenizer.batch_encode_plus(X_val.tolist(), \n",
    "\t\t\t\t\t\t\t\t\t\t\tpadding=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmax_length = max_len,\n",
    "\t\t\t\t\t\t\t\t\t\t\treturn_tensors='tf')\n",
    "\n",
    "X_test_encoded = tokenizer.batch_encode_plus(X_test.tolist(), \n",
    "\t\t\t\t\t\t\t\t\t\t\tpadding=True, \n",
    "\t\t\t\t\t\t\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tmax_length = max_len,\n",
    "\t\t\t\t\t\t\t\t\t\t\treturn_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Intialize the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "146/146 [==============================] - 1601s 11s/step - loss: 1.3649 - accuracy: 0.3080 - val_loss: 1.3430 - val_accuracy: 0.3648\n",
      "Epoch 2/3\n",
      "146/146 [==============================] - 1351s 9s/step - loss: 1.3233 - accuracy: 0.3733 - val_loss: 1.2627 - val_accuracy: 0.4549\n",
      "Epoch 3/3\n",
      "146/146 [==============================] - 1364s 9s/step - loss: 1.2129 - accuracy: 0.4901 - val_loss: 1.2059 - val_accuracy: 0.5172\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train the model\n",
    "history = model.fit(\n",
    "\t[X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
    "\ty,\n",
    "\tvalidation_data=(\n",
    "\t[X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],y_val),\n",
    "\tbatch_size=32,\n",
    "\tepochs=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 38s 2s/step - loss: 1.1353 - accuracy: 0.5236\n",
      "Test loss: 1.1353317499160767, Test accuracy: 0.5236051678657532\n",
      "Training accuracy: 0.4901202619075775, Validation accuracy: 0.5171673893928528\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "\t[X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
    "\ty_test\n",
    ")\n",
    "\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last training accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last validation accuracy\n",
    "\n",
    "# Print the results\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "print(f'Training accuracy: {train_accuracy}, Validation accuracy: {val_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 33s 2s/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "logits = model.predict([X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5771451838762672\n",
      "Recall: 0.5236051502145923\n",
      "F1 Score: 0.47864764317974173\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "\n",
    "# If it's binary classification, use a threshold of 0.5 on the positive class probability\n",
    "if y_pred_probs.shape[1] == 2:\n",
    "    y_pred = (y_pred_probs[:, 1] > 0.5).astype(int)\n",
    "else:\n",
    "    # For multi-class classification, take the argmax of the probabilities\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y back into DataFrames for each split\n",
    "# train_df = pd.DataFrame({'Preprocessed text': X_train.to_numpy(), 'sentiment': y_train.to_numpy()})\n",
    "# val_df = pd.DataFrame({'Preprocessed text': X_val.to_numpy(), 'sentiment': y_val.to_numpy()})\n",
    "# test_df = pd.DataFrame({'Preprocessed text': X_test.to_numpy(), 'sentiment': y_test.to_numpy()})\n",
    "\n",
    "# # Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# val_dataset = Dataset.from_pandas(val_df)\n",
    "# test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     # Check if 'text' is a list, and tokenize accordingly\n",
    "#     return tokenizer(examples[\"Preprocessed text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# # Assuming 'data' is your dataset containing the text data\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Set the format for PyTorch\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n",
    "# test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n",
    "# val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import accelerate\n",
    "# import torch\n",
    "\n",
    "# print(\"Transformers version:\", transformers.__version__)\n",
    "# print(\"Accelerate version:\", accelerate.__version__)\n",
    "# print(\"Torch version:\", torch.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",                 # Output directory for model predictions and checkpoints\n",
    "#     evaluation_strategy=\"epoch\",             # Evaluate after each epoch\n",
    "#     learning_rate=2e-5,                      # Learning rate\n",
    "#     per_device_train_batch_size=16,         # Batch size for training\n",
    "#     per_device_eval_batch_size=16,          # Batch size for evaluation\n",
    "#     num_train_epochs=3,                      # Total number of training epochs\n",
    "#     weight_decay=0.01,                       # Weight decay for regularization\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                             # Your model instance\n",
    "#     args=training_args,                      # Training arguments\n",
    "#     train_dataset=train_dataset,             # Training dataset\n",
    "#     eval_dataset=val_dataset                 # Validation dataset\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
